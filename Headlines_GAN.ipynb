{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Finding an Expansive View  of a Forgotten People in Niger', 'And Now,  the Dreaded Trump Curse', 'Venezuelaâ€™s Descent Into Dictatorship', 'Stain Permeates Basketball Blue Blood', 'Taking Things for Granted']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = 'nyt/'  # path to dataset\n",
    "\n",
    "all_headlines = []\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if 'Article' in filename:\n",
    "        headlines = pd.read_csv(path+filename)\n",
    "        all_headlines.extend(list(headlines.headline.values))\n",
    "\n",
    "all_headlines = [hline for hline in all_headlines if hline != 'Unknown']\n",
    "print(all_headlines[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train = tfidf.fit_transform(all_headlines)\n",
    "X_train = X_train.toarray()\n",
    "X_train = tf.convert_to_tensor(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "batch_size = 512\n",
    "vocab_size = noise_dim = 10616 \n",
    "max_length = 20    # Maximum number of words in a headline\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(BUFFER_SIZE).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Dense(256, use_bias=False, input_shape=(vocab_size,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Dense(512, use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(layers.Dense(1024, use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    output_size = vocab_size\n",
    "    model.add(layers.Dense(output_size, activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.InputLayer(input_shape=(vocab_size,)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Dense(256))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Dense(128))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='tanh'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions and optimizers\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "num_examples_to_generate = 5\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function  # for compiling\n",
    "def train_step(txt):\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "    \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_txt = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(txt, training=True)\n",
    "      fake_output = discriminator(generated_txt, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_texts(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    \"\"\"\n",
    "    Uncomment below to get all predictions\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # for prediction in predictions:\n",
    "    #     top_indices = tf.math.top_k(prediction, k=10).indices.numpy()\n",
    "    #     top_words = [tfidf.get_feature_names_out()[i] for i in top_indices]\n",
    "    #     print(\" \".join(top_words).title())\n",
    "\n",
    "    \"\"\"\n",
    "    Or keep this to only get predictions found realistic by the discriminator\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for pair in zip(predictions, discriminator(predictions)):\n",
    "        if pair[1] > 0:\n",
    "            top_indices = tf.math.top_k(pair[0], k=10).indices.numpy()\n",
    "            top_words = [tfidf.get_feature_names_out()[i] for i in top_indices]\n",
    "            print(\" \".join(top_words).title())\n",
    "\n",
    "\n",
    "\n",
    "def generate_last(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False)\n",
    "    for prediction in predictions:\n",
    "        top_indices = tf.math.top_k(prediction, k=10).indices.numpy()\n",
    "        top_words = [tfidf.get_feature_names_out()[i] for i in top_indices]\n",
    "        print(\" \".join(top_words).title())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for text in dataset:\n",
    "      train_step(text)\n",
    "\n",
    "    generate_and_save_texts(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print(f'Epoch {epoch + 1} completed in {time.time()-start:.2f} secs. \\n')\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  generate_last(generator,\n",
    "                epochs,\n",
    "                seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed in 18.11 secs. \n",
      "\n",
      "Epoch 2 completed in 12.03 secs. \n",
      "\n",
      "Epoch 3 completed in 10.62 secs. \n",
      "\n",
      "Epoch 4 completed in 10.41 secs. \n",
      "\n",
      "Epoch 5 completed in 11.20 secs. \n",
      "\n",
      "Explained Vuong Gulf Pilot Gouging Saying Diary Roseanne Lin Loss\n",
      "Rebecca Roosevelt Later Duterte Religious Simplicity Feather Plucky Jousting Besties\n",
      "Dividing Sublet Mishap Trigger Alike Newest Valentine Difference Believed Brief\n",
      "Plot Bee Sarin Buyer Coelacanth Enhanced 20 Kipling Elderly Feminism\n",
      "Greenpeace Milwaukee Dismantling Task Ability He Verse Draw Vendor Anything\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
